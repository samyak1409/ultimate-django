## What is Automated Testing

- **Problem with Manual Testing**:

  - Time required grows exponentially as application complexity increases.
  - Rules behind endpoints may be forgotten over time.
  - Documenting rules to be used by testers still leads to repetitive, time-consuming processes.

- **Solution: Automated Testing**:

  - Write code once to test endpoints and rules.
  - Tests can be run repeatedly after code changes or before deployments.
  - Hundreds/thousands of tests execute in seconds.
  - Helps catch regressions early.
  - Leads to better, more reliable code and confident releases.

- **Mindset Issues**:

  - Some developers dismiss automated testing as wasteful.
  - Automated testing is a "double-edged sword":
    - Done right → extremely valuable.
    - Done wrong → slows development, adds frustration.
  - Many poor practices and misinformation exist.
  - Right way to write tests is discussed next.

## Test Behaviours, Not Implementations

**Key Principle**: Many fail at automated testing because they test **implementations** instead of **behaviors**.

- **Software Testing Analogy**:

  - Test **how software behaves**, not its internal implementation.
  - Implementation details (models, views, routers, serializers) may change over time.
  - If tests are tied to implementation, changes will break tests unnecessarily, leading to wasted effort fixing them.

- **Behavior-Driven Testing Example: `collections` endpoint**:

  - **Scenario 1:** Client not authenticated → expect **401 Unauthorized**.
  - **Scenario 2:** Authenticated, but user not admin → expect **403 Forbidden**.
  - **Scenario 3:** Admin but request missing collection name → expect **400 Bad Request** with error message.
  - **Scenario 4:** Admin with collection name provided → expect **200 OK** and new collection ID in response.

- **Summary**:

  - Behavior can be checked manually (browser, API client) or automated with code.
  - Automated tests = fast, repeatable, reliable way to detect breakage.
  - Core takeaway: **Test the behaviour, not the implementation**.

## Tooling

- **Need for test frameworks**

  - Provide structure for writing tests
  - Provide runner & reports

- **Popular Python frameworks**

  - `unittest` → Python built-in
  - `pytest` → separate install, widely preferred

- **Why pytest is better**

  - More features
  - Large community + plugins
  - Less boilerplate → shorter, cleaner, more concise tests

<details>
<summary>See example comparison</summary>

- **Using `unittest` (built-in)**

  ```py
  import unittest

  def add(a, b):
      return a + b

  class TestMath(unittest.TestCase):
      def test_add(self):
          result = add(2, 3)
          self.assertEqual(result, 5)

  if __name__ == "__main__":
      unittest.main()
  ```

- **Using `pytest`**

  ```py
  def add(a, b):
      return a + b

  def test_add():
      assert add(2, 3) == 5
  ```

- **Key Differences**

  - **`unittest`**: requires a test class, method definitions, and `self.assert*` methods.
  - **`pytest`**: simpler, no class needed, just plain `assert`.

<br></details>

- **Installation (development dependency only)**

  - `pipenv install pytest --dev`
  - Adds pytest under `dev-packages` in `Pipfile`

- **Django-specific**

  - Need pytest plugin: `pipenv install pytest-django --dev`

Docs: [pytest](https://docs.pytest.org), [pytest-django](https://pytest-django.readthedocs.io)

## Your First Test

- **Folder & file conventions**

  - Create a `tests` folder (plural is REQUIRED by convention).
  - Test file names must start with `test_`. (e.g. `test_collections.py`)

- **Function naming convention**

  - Test function names must start with `test_`.
  - Name should describe the scenario being tested (not vague like `test_collections` or `test_create`).
  - Example: `test_if_user_is_anonymous_returns_401`.

- **Organizing tests**

  - Use classes to group related tests by use case.
  - Class name must start with `Test`.
  - Example: `TestCreateCollection`.

- **Test structure (AAA pattern)**

  - **Arrange** – prepare system (objects, DB state, etc.).
  - **Act** – perform the action (e.g., send request).
  - **Assert** – check the result matches expectations.

- **Test #1**

  ```py
  from rest_framework.test import APIClient
  from rest_framework import status

  class TestCreateCollection:

      def test_if_user_is_anonymous_returns_401(self):

          # Arrange
          client = APIClient()

          # Act
          response = client.post('/store/collections/', {'title': 'X'})

          # Assert
          assert response.status_code == status.HTTP_401_UNAUTHORIZED
  ```

  - Always end URLs with `/` (Django REST requires it).
  - Keep test data minimal (e.g., `"X"` instead of long string "Tech Products").

## Running Tests

Create config file `pytest.ini` in project root:

```ini
[pytest]
DJANGO_SETTINGS_MODULE = storefront.settings
```

Run `pytest` → executes tests.

- **Validating tests (avoid "lying tests"):**

  - Comment out exactly the part of the code responsible for returning `401`.
  - Run test → should fail if test is valid.
  - If still passes → it's a "lying test" (bad test).

- **Database access issue:**

  - Without permission, endpoint tries to create collection → needs DB access.

  - Pytest blocks DB by default → must mark tests.

  - Add decorator (at selected methods or class itself):

    ```py
    import pytest

    @pytest.mark.django_db
    class TestClass
    ```

- **Assertion verification:**

  - Expected `401`, got `201` → confirms test is valid.
  - Restoring permission line fixes test.

- **Importance of tests:**

  - Tests validate **behavior**, not implementation details.
  - Implementation can change (e.g., switch to function-based view), tests still pass if behavior is correct.
  - Tests serve as living documentation of requirements.

- **Running selective tests:**

  - `pytest`: Runs all
  - Run app: `pytest store`
  - Run directory: `pytest store/tests`
  - Run module: `pytest store/tests/test_collections.py`
  - Run class: `pytest store/tests/test_collections.py::TestCreateCollection`
  - Run method: `pytest store/tests/test_collections.py::TestCreateCollection::test_if_user_is_anonymous_returns_401`
  - Run by keyword: `pytest -k 'anonymous'`  
    Runs all test functions & classes whose name contains 'anonymous'.

## Skipping Tests

- Sometimes tests fail but fixing them immediately may distract from current work.
- To **temporarily skip** such tests, add `@mark.skip` to that test.
- Running pytest shows skipped tests (`1 skipped`), without marking them as failures.
- This reduces noise and allows you to continue working. Later, revisit and fix the skipped tests.

## Continuous Testing

- **Two ways to run tests**:

  - _[Mandatory]_ On demand (before committing/deploying).
  - _[Optional]_ Continuously (tests rerun automatically while coding).

- **Continuous testing**:

  - Runs tests automatically whenever code changes.
  - Useful for immediate feedback (know if something broke while coding).
  - May slow down performance on slower machines.
  - Common practice: dedicate a separate terminal/monitor for running tests.

- **Setup with pytest**:

  - Install plugin:

    ```sh
    pipenv install pytest-watch --dev
    ```

  - Run with:

    ```sh
    pytest-watch
    ```

    (Or just: `ptw`)

- **Behavior**:

  - Runs all tests initially.
  - Automatically reruns tests when code or test files change.
  - Provides real-time notifications of failures or successes.

- **Best practice**: Always run tests before committing code and before deployment.

## Running and Debugging Tests in VSCode

- **Testing Panel (Test Explorer)** (Conical flask icon) in VS Code can run/debug tests.

- **Setup:** Click Conical flask icon → _Configure Python Tests_ → _pytest_ → _. Root directory_

- **Capabilities in Test Explorer:** Run/Debug individual/specific tests.

- **Debugging:** Add a **breakpoint** in test code, and click debug test via Test Explorer.

## Authenticating the User

> Very important lesson.

- **Test #2**: Test when user is authenticated but is not an admin.

- **Setup**:

  Similar to the test we wrote above. Only differences:

  - Use `client.force_authenticate(user)` to simulate authentication. `user` is a non-admin user object.
  - Expected response → **403 Forbidden** (user lacks permission).

### Major mistake by Mosh (Lying test)

- Mosh used:

  ```py
  client.force_authenticate(user={})
  ```

  But:

  > In Django REST Framework, `force_authenticate(user)` expects a **User object**, not an empty dictionary.

- **But using `{}` still passes the test, this is because:**

  - DRF sets `request.user = {}`.
  - Since `{}` has no `is_authenticated`, permission checks fail → we get **403 Forbidden**.
  - This is an **accidental hack**, not the right way.

  We can verify that this test is a "**lying test**" (we learned this above only in `Running Tests` topic) by:

  - Changing the exact part by which the test is succeeding. (No change in the test.)

  - That part is `permission_classes = [custom_permissions.IsAdminOrReadOnly]` (in `store.views.CollectionViewSet`). Change it to `permission_classes = [permissions.IsAuthenticated]`.

  - And we can see the test is still passing.

    If `force_authenticate(user={})` is actually creating a (non-admin) user, then since we now have `permission_classes = [permissions.IsAuthenticated]`, the test should've failed since `assert 201 == 403`.

- **So, what's the correct way?**

  Using `user.objects.create_user()`:

  ```py
  from django.contrib.auth.models import User

  user = User.objects.create_user(username='testuser', password='testpass')
  client.force_authenticate(user=user)
  ```

  But since we're using our custom `User` model (`core.models.User`):

  (`from core.models import User` -> makes the store app dependent on the `core` app -> bad coupling)

  (Can't use `from django.conf import settings`, `settings.AUTH_USER_MODEL`, since it's just a `str` not actual reference.)

  ```py
  from django.contrib.auth import get_user_model

  user = get_user_model().objects.create_user(username='testuser', password='testpass')
  client.force_authenticate(user=user)
  ```

  With this in place, now our test passes the "lying test" test.

- **Wouldn't this create a new user every time the test is ran?**

  > Pytest creates a **temporary test database** (`test_<dbname>`) and deletes it after tests finish.
  >
  > With `@pytest.mark.django_db`, each test runs inside a transaction and Django rolls back after the test — so the DB is clean after each test.
  >
  > So:
  >
  > - **Test run scope** → pytest creates and deletes a test database.
  > - **Test function scope** → Django uses transactions to isolate each test.

References: [ChatGPT](https://chatgpt.com/share/68b36214-60c8-800a-8adf-f0ebc5830b17), [Gemini](https://g.co/gemini/share/510bcaf8e42a)

- **P.S.:**

  Since in our current tests, we aren't doing anything with the user, we don't actually need to save it in the DB, we can just use an in-memory dummy user as well:

  ```py
  User = get_user_model()
  client.force_authenticate(user=User())
  ```

  Most of the time though, we use `create_user` only.

References: [ChatGPT](https://chatgpt.com/s/t_68b368cca1e081919fc47a3fb96f33b6), [Gemini](https://g.co/gemini/share/14ff28c31b01)

## Single or Multiple Assertions

- **Test #3: Authenticated admin, invalid data**

  - Use `User(is_staff=True)` for authentication (admin).

  - Post invalid data (e.g., `title=''`).

  - Assertions:

    - `status_code == 400 BAD REQUEST`.

    - Response body includes `title` error: `'title' in response.data`  
      (Don’t check exact wording because that's an implementation detail, which might change.)  
      (Why `title` is returned? Because that's the field which was posted with bad data.)

- **Multiple Assertions in a Test**

  - Principle: A test should have **single responsibility**, not necessarily single `assert`.
  - Multiple logically related assertions are fine (e.g., status code + response body).
  - Splitting them into separate tests would duplicate code with no added value.

- **Test #4: Authenticated admin, valid data**

  - Post valid data (`title='X'`).

  - Assertions:

    - `status_code == 201 CREATED`.
    - Response includes `id` and it should be `> 0`: `response.data["id"] > 0`

- **Alternative approaches to checking id > 0 (and why they’re not recommended):**

  - **Query DB with model**:

    - Example: `Collection.objects.get(id=...)`.
    - Problem: Couples test to implementation (model).
    - If model changes, test breaks unnecessarily.
    - Principle: _The less tests know about internals, the more reliable they are_.

  - **Send GET request to collections endpoint**:

    - Problem: If the endpoint has a bug, the test fails even though creation succeeded.
    - Leads to **false negatives** and noisy test failures.

- **Checking `id > 0` guarantees DB save?**

  - Not strictly true: the API could, in theory, return an `id` without actually saving to DB.
  - In practice, with Django REST Framework, if you’re hitting a properly configured `ModelViewSet`/serializer, returning `id` implies the object was saved. So the test is reliable enough—but not a 100% theoretical guarantee.
  - This approach works because we trust DRF’s serializer + model save pipeline.

  \- [ChatGPT](https://chatgpt.com/s/t_68b37625883c8191a32b02d9415e8b8d)

## Fixtures

> **VERY IMPORTANT, PAY ATTENTION TO SMALL DETAILS**

**Why Fixtures?**

- Remove duplication in test code.
- Example: every test is creating an `APIClient` and importing it repeatedly → use fixtures to simplify.

### Global Fixtures (`conftest.py`)

- `conftest.py` is a **special file** loaded automatically by `Pytest`.

- Define reusable fixture:

  ```py
  from rest_framework.test import APIClient
  import pytest

  @pytest.fixture
  def api_client():
      return APIClient()
  ```

- Now in tests, just add `api_client` as a parameter.

> What's the advantage of using pytest fixture when we can achieve the same using functions?
>
> Read: [_Pytest Fixtures: Power Beyond Functions_ - Gemini](https://g.co/gemini/share/92c7a9e60f24)

### Local Fixtures (module-specific)

- Fixtures specific to one test module should be defined in that module (not in `conftest.py`).

- Example: `create_collection` fixture.

  - Needs `api_client`.

  - Returns an **inner function** (`create`) that takes collection title and posts it.

  - This is a **closure** → function returning another function. (This concept exists in JavaScript as well.)

    ```py
    @pytest.fixture
    def create_collection(api_client):
        def create(title):
            return api_client.post(path="/store/collections/", data={"title": title})
        return create
    ```

    Why do we need an inner function? Try doing this without it, you'd get the answer.

    ```py
    @pytest.fixture
    def create_collection(api_client, title):
        return api_client.post(path="/store/collections/", data={"title": title})
    ```

    The problem is `create_collection` expects `title` to be a fixture as well! Not an arg.

  - Usage in test:

    ```py
    def test_example(create_collection):
        response = create_collection(title="X")
        assert ...
    ```

### [Global] Fixture for Authentication

- Repeated pattern: authenticating a user.

- Define global fixture in `conftest.py`:

  ```py
  User = get_user_model()

  @pytest.fixture
  def authenticate(api_client):
      def func(is_staff=False):
          api_client.force_authenticate(user=User(is_staff=is_staff))
      return func
  ```

- Usage in tests:

  ```py
  def test_example(authenticate):
      authenticate(is_staff=True)
      response = ...
      assert ...
  ```

Also, **IMPORTANT**:

Notice that since `api_client` fixture is only initialized once, that's only why we are able to `authenticate` and `create_collection` without sharing the `api_client` object explicitly, and it's still working!

## Creating Model Instances

### Test Scenarios for Retrieving a Collection

1. **Collection does not exist →** Expect `404 Not Found`.
2. **Collection exists →** Expect `200 OK` and the collection returned in the response body.

### Test Organization

- Create a test class (e.g., `TestRetrieveCollection`).
- Use `@pytest.mark.django_db` because database access is needed.
- Each test should be **independent** (tests must not depend on one another).

### Creating Test Data

**Two options to create a collection for the test**:

1. **POST request via API** → Wrong, because if "create collection" is buggy, retrieval tests also fail.

2. **Directly use model (Collection.objects.create)** → Correct.

   > **IMPORTANT:** Touching models in `Arrange` step is okay, as long as your `Assert` checks only behavior.

### Model Bakery

- Library for easily creating model instances for tests.

- Install: `pipenv install model-bakery --dev`.

- Usage:

  - `from model_bakery import baker`

    `baker.make(Collection)` → creates a collection with auto-generated field values.

  - Auto-fills fields depending on type (string, datetime, etc).

  - Handles relationships automatically.

    Example: `baker.make(Product)` will create a `Collection` first.

  - Create multiple objects: `baker.make(Product, _quantity=10)`.

  - To place them in the **same collection**:

    ```py
    collection = baker.make(Collection)
    products = baker.make(Product, collection=collection, _quantity=10)
    ```

Alternate to [model_bakery](https://github.com/model-bakers/model_bakery): **[pytest-factoryboy](https://github.com/pytest-dev/pytest-factoryboy)**

- **model-bakery** → Quick, zero-boilerplate, auto-generates valid data; great for **small/medium projects or prototyping**, but less reusable/customizable.
- **pytest-factoryboy** → Structured, reusable, fine-grained control over test data; good for **large/complex projects**, but requires writing factories (more setup).

  Src: [ChatGPT](https://chatgpt.com/share/68b45985-15d4-800a-a8eb-2890c8700a9d), [Gemini](https://g.co/gemini/share/86352d2e4371)

### Writing the Retrieval Test

`test_if_collection_exists_returns_200`

- Arrange: create a collection (`collection = baker.make(Collection)`).

- Act: `response = api_client.get(f'/store/collections/{collection.id}/')`

  - Don't forget the **trailing slash** else the test will fail.
  - Django auto-redirects missing trailing slash, so response status changes to `301 Permanent Redirect`.

- Assert:

  - `assert response.status_code == status.HTTP_200_OK`
  - Compare response body directly:

    ```py
    assert response.data == {
        "id": collection.id,
        "title": collection.title,
        "products_count": 0
    }
    ```

<br>

**Exercise: Collection does not exist**

```py
def test_if_collection_does_not_exists_returns_404(self, api_client):

    response = api_client.get(path=f"/store/collections/1/")
    # any id would work since in the test db, no id would be present

    assert response.status_code == status.HTTP_404_NOT_FOUND
```

**Important lesson:**

**Problem:** Id `1` should've given status `404`, but it was giving `200`, meaning id `1` is present in the test DB.

**Cause:**

Turns out in `3. Setting Up the Database.md > Running Custom SQL`, we left the custom migration containing:  
`"INSERT INTO store_collection (title) VALUES ('collection1')"`

And here's what actually happens with `@pytest.mark.django_db`:

1. **Database Creation:** For a test run, `pytest-django` creates a brand-new, empty test database.
2. **Migrations Run:** It then runs **all of your project's migrations** on this new database. This happens only once per test session.
3. **Test Execution:** Each test function is then wrapped in a database transaction. Anything your test function does to the database is rolled back after the test finishes, ensuring tests don't interfere with each other.

Hence, the `200`.

**Solution:** Create a **new** migration at the end of the chain that reverses the changes from the bad migration.

Ref: [Gemini](https://g.co/gemini/share/c61fa36c717e)
