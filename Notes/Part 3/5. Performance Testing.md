## Why Performance Testing

- **Performance testing is essential** during development, not just before production.
- Many companies skip or postpone it due to deadlines → leads to production failures.
- Ignoring performance issues means they will eventually surface.
- Performance testing helps identify and fix costly/disastrous issues early.
- The topic is **broad and complex** (entire books exist), but even basic tools can uncover most common performance problems.

## Installing Locust

**Locust** is:

- Simple to use.
- Has a nice UI.
- Allows writing tests in **Python**.

```sh
pipenv install --dev locust
```

## Creating a Test Script

Docs: [Writing a locustfile](https://docs.locust.io/en/stable/writing-a-locustfile.html)

### What to Performance Test?

- Identify **core use cases** (business-critical user flows).

- Example core use cases for our app:

  - Browsing products catalog.
  - Viewing product details.
  - Registering, signing in/out.
  - Adding products to cart.

### Setup

- Create a `locustfiles/` folder (name can be anything but this is convention).
- Inside, create a test file (e.g. `browse_products.py`).
- Define a class extending **`HttpUser`**.
- Locust creates an instance of this class per simulated user.
- Define **tasks** as methods decorated with `@task`.

Structuring: (Read TLDR) [Gemini](https://g.co/gemini/share/b15fd1963cdb), [ChatGPT](https://chatgpt.com/share/68b6f7a2-c83c-800a-bcfc-ee00367cac26)

### Example Tasks

1. **Browse products**

   - `self.client.get(url="/store/products/?collection_id=<random_id>")`
   - Use random collection ID (e.g. 1–10).
   - Use `name="store/products"` to group in reports (since the url contains a dynamic value).

2. **View a single product**

   - Random product ID (1–1000).
   - GET `/store/products/<id>/`

3. **Add product to cart**

   - Limit random product ID range (e.g. 1–10) to simulate duplicates.
   - Requires a **cart ID**, created at user start (using `on_start()`, see `Lifecycle Hook` below).
   - POST to `/store/carts/<id>/items/`
   - JSON payload: `{"product": <id>, "quantity": 1}`

- Important: **Always end URLs with a trailing slash (`/`)**, sometimes Django redirects and skews performance results.

### Lifecycle Hook

- `on_start()` runs when a user is "spawned" (created).
- Create a cart via `self.client.post("/store/carts/")`.
- Save `id` from response in `self.cart_id`.

### Weights & Timing

- Assign **weights** (pass in `@task()`):

  - Browse products → `weight=2`
  - View product → more frequent → `weight=4`
  - Add to cart → less frequent → `weight=1`

- Simulate human delay between **tasks**:

  Add class attribute:

  ```py
  wait_time = between(1, 10)
  ```

<br>

> Good practice: Rerun performance tests after code changes.

Summary: Locust simulates real-world user flows (browse, view, add to cart), assigns weights, groups requests in reports, and adds wait times for realism.

## Running a Test Script

- **Running Locust**

  - Make sure target website is live.
  - In new terminal: `locust -f <file_name>.py`.
  - On any code changes, re-run locust.
  - Locust UI runs at [localhost:8089](http://localhost:8089).

- **Web UI: Input parameters:**

  - **Number of users** = peak concurrency.
  - **Ramp up** = users started/second.
  - **Host** = e.g. [127.0.0.1:8000](http://127.0.0.1:8000) (without trailing `/` since URLs in test script start with `/`).  
    Adding this every time can be avoided by adding class attribute `host = 'http://127.0.0.1:8000'` to the user class.

- **Statistics Page**

  - Shows requests per endpoint: GET all products, GET single product, POST create cart item.
  - Since **grouping** is applied in script (using `name` param), requests are grouped under one name.
  - Displays metrics:
    - Request count.
    - Failures.
    - Median, 90 percentile, average, min, max response times.
    - Response size.
    - Current requests per second (RPS).

- **Other Features**

  - Request rate and response time charts update live.
  - Shows connection failures or exceptions.
  - Task ratio available.
  - Data downloadable as CSV.

> Raised an issue: [Reset button not working after stopping the run](https://github.com/locustio/locust/issues/3197)

## Running a Performance Test

- Setup:

  - Deliberately introduced a performance issue by removing `prefetch_related` from `ProductViewSet`.
  - For performance/load testing, **set `DEBUG=False`** so that tools like Debug Toolbar, Silk are not active, as they add overhead & skew results. (Won't be active only if they're added using `if DEBUG`.)

- Test Configuration:

  - 500 users total, spawn rate = 50 users/sec.
  - Ran for ~30s.

- Note:

  - Numbers are not absolute indicators of production performance.  
    Only use them for **relative comparisons** across endpoints/tests.
  - Numbers vary across runs and differ between dev and production.  
    Django dev server is slower and not production-grade.
  - Endpoints significantly slower than others → needs optimization.

## Performance Optimization Techniques

- **Performance testing helps find slow endpoints.**

- **Common bottleneck**: Database/queries (90% cases).

**Optimization Techniques (In order ↓)**

- **Optimize ORM usage**

  - Use `select_related` / `prefetch_related` → reduce extra queries.
  - Use `only()` → fetch only required fields.
  - Use `defer()` → skip heavy fields not needed immediately. (See [Django ORM > Deferring Fields](https://github.com/samyak1409/ultimate-django/blob/main/Notes/Part%201/4.%20Django%20ORM.md#deferring-fields))
  - Use `values()` / `values_list()` → return (QuerySet of) dicts/lists instead of (QuerySet of) full model objects (cheaper).
  - Use `count()` instead of fetching all objects (`all()`) and calling `len()`.
  - Use `bulk_create()` / `bulk_update()` instead of looping inserts/updates → fewer DB roundtrips.

- **Use Raw SQL**

  - If ORM-generated queries are inefficient → **rewrite in raw SQL**.

- **Tune DB**

  - Redesign tables, add indexes, optimize schema.

- **Cache the result**

  Store query results in memory. (Redis)

  - Speeds up repeated queries.
  - Not always faster → cache access can be slower than a simple query (esp. over network to a cache server).

- **Scale (More hardware)**

  - Scale up (Vertical scaling) → better CPU/RAM.
  - Scale out (Horizontal scaling) → add more servers.

<br>

**Remember:**

- Not all parts of app need maximum speed.
- Focus on **critical paths** (frequent operations, user-facing features).
- Low-frequency tasks (e.g., admin report every few months) don’t need heavy optimization.

## Profiling with Silk

- **Purpose:**

  - Profiling tool for Django.
  - Helps identify slow endpoints, queries, and request/response lifecycle.
  - Collects execution profiles: functions, queries, timings.

- **Installation & Setup:**

  Note: Silk should never be used in production — it adds _significant_ overhead.  
  So add middleware, installed app, and url pattern under **`if DEBUG`**.

  1. `pipenv install --dev django-silk`
  2. Add middleware (`'silk.middleware.SilkyMiddleware'`) → placement matters.
     - Middleware processes requests in sequence.
     - If earlier middleware ends request, Silk won’t see it.
     - So best to put Silk **first**.
  3. Add `'silk'` to `INSTALLED_APPS`.
  4. Add URLs: `path('silk/', include('silk.urls', namespace='silk'))`
  5. `python manage.py migrate`

  Ref: [GitHub](https://github.com/jazzband/django-silk?tab=readme-ov-file#installation)

- **Usage:**

  - Run a load test (500 users) → Silk intercepts requests & collects data.
  - Dashboard shows:
    - **Most Time Overall** (slowest endpoints).
    - **Most Time in Database.**
    - **Most DB Queries.**
    - Request/response details (sorting, execution times, params, headers, etc.).
    - SQL tab → exact queries executed, execution time, etc.

- **Example Findings:** `/store/products/` endpoint → 13 queries (due to removing `prefetch_related` in `ProductViewSet`).

  **Fix:** Add `prefetch_related` back in `ProductViewSet`.

## Verifying Optimizations

- **Testing Process**: Use the same load test params as before.

- **Results**: Reduced response times.

- **Takeaway**: Always benchmark before/after optimization.

## Stress Testing

A performance test to find the _upper limit_ where the application fails.

- **Why important**:

  - Helps determine application + environment capacity.
  - Useful during expected traffic spikes (e.g., flash sales).

Stress tests should be run on staging or production-like environments. The dev server is only for demo purposes.
