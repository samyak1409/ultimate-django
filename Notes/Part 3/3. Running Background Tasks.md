## Introduction to Celery

- **Why Background Tasks?**

  - Some tasks are resource-intensive (CPU or I/O): image/video processing, report generation, email sending, ML model execution.
  - Running them in the main app process blocks requests and slows response times.
  - Solution: Offload heavy tasks to **background workers**.

- **Real Example**

  - User uploads a video.
  - Instead of processing in the main app (slow), app quickly acknowledges upload and processes in background.
  - Once done, notify user: "Your video is processed and ready."

- **Celery**

  Website: [docs.celeryq.dev](https://docs.celeryq.dev)

  - Tool for **asynchronous task processing**.
  - Can run **multiple workers** in parallel.
  - Tasks are placed in a **queue**.
  - Workers watch the queue → pick tasks → execute → become free for next task.

- **Scalability**

  - Can add more workers to handle higher load.
  - Main application process remains unaffected by delays or failures.

- **Additional Feature**

  - Celery can schedule **periodic tasks** (like [cron jobs](https://www.google.com/search?q=cron+jobs)) using `celery beat`.
  - Example: run every hour, or every Sunday at 12 a.m.

- **Key Benefit**

  - Keeps main app responsive while heavy tasks run in the background.

## Message Brokers

Docs: [Choosing a Broker](https://docs.celeryq.dev/en/stable/getting-started/first-steps-with-celery.html#choosing-a-broker)

> Celery requires a solution to send and receive messages; usually this comes in the form of a separate service called a message broker.

- **Message Broker**: Acts as a middleman to reliably deliver messages between applications.

  - If target app is down, broker stores message and retries later.
  - What if broker itself went down? Brokers can be clustered for fault tolerance.

- **Why needed**: Django app must reliably send tasks/messages to Celery workers.

- **Most Popular Brokers**:

  - **RabbitMQ**:

    - A full-fledged enterprise-grade broker.
    - More features, but also more complexity.

  - **Redis**:

    - Not a true broker (an in-memory data store), but using it with Celery is extremely common and reliable for most apps.
    - Can be used as a broker and also a cache.
    - Easy to set up → recommended to start with.

  Also, see: [Backends and Brokers » Summaries](https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/index.html#summaries)

- **RabbitMQ vs Redis**:

  - Redis is simpler but has fewer enterprise-level features like advanced routing, acknowledgments, etc.
  - RabbitMQ is more complex, but not necessarily “better” unless your system truly requires those features.

- **Recommendation**: Start with Redis (simple, fast to set up). Switch to RabbitMQ only if Redis doesn’t meet requirements.

> Redis is also used as a caching layer, see `Caching` section.

## Installing Redis

Docs: [Redis](https://docs.celeryq.dev/en/stable/getting-started/first-steps-with-celery.html#redis)

- **Tool required**: Docker (already installed in `Sending Emails » Setting up a Fake SMTP Server` section).

- **Command**:

  ```sh
  docker run -d -p 6379:6379 redis
  ```

  - `-d`: run container in detached (background) mode.
  - `-p`: map host port to container port (here `6379:6379`).
  - `redis`: the image to run (docker automatically downloads from Docker Hub if not present).

- **Concepts explained**:

  - A Docker container = a process on your machine, isolated like a lightweight virtual machine.
  - Port mapping allows accessing the container from the host machine.
  - Port `6379` is Redis’s default listening port.

- **Verification**:

  `docker ps` shows running containers, with ID, status, and mapped ports.

  - To stop:

    ```sh
    docker stop <CONTAINER ID>
    ```

    (Quick tip: No need to give the whole ID, just type initial chars such that the prefix is unique.)

- **In Django project**:

  Install Redis Python client ([redis-py](https://github.com/redis/redis-py)):

  ```sh
  pipenv install redis
  ```

## Celery and Windows

<details>
<summary>For Windows users.</summary>

- **Celery Windows Support**: Dropped since Celery version 4.
- **Implication**: Cannot run Celery natively on Windows.
- **Solution**: Run Django project inside a Linux environment using **WSL (Windows Subsystem for Linux)**.
- **WSL Environment**: Has its own file system, but is easy to set up even for Linux beginners.
- **Setup Guide**: Instructor provides a PDF with step-by-step instructions.
- **Time Required**: Setup may take 30–60 minutes or longer.
- **Action Step**: Stop here, follow the WSL setup instructions, then continue.

</details>

## Setting Up Celery

**[Install](https://docs.celeryq.dev/en/stable/getting-started/first-steps-with-celery.html#installing-celery):**

```sh
pipenv install celery
```

<br>

Docs: [First steps with Django » Using Celery with Django](https://docs.celeryq.dev/en/stable/django/first-steps-with-django.html)

1. **Create `celery.py` besides `settings.py` or `settings/`**

   - Set env var.
   - Create Celery instance.
   - Configure with Django settings.
   - Set auto-discover `tasks.py` file(s) (which would actually contain our tasks).

   See the code in `storefront/celery.py`.

2. **Configure Celery settings in `settings.py`**

   ```py
   CELERY_BROKER_URL = "redis://localhost:6379/0"
   ```

   Redis uses numbered databases (0 to 15 by default, 16 total).

3. **Load Celery app in `__init__.py`**

   ```py
   from .celery import app as celery_app
   ```

   (So that when we start/run the project, celery is started.)

4. **Start Celery worker**

   ```sh
   celery --app storefront worker --loglevel=info
   ```

   - `--app storefront`: points to project.
   - `worker`: runs worker process. (No. of workers equal to CPU cores, by default.)
   - `--loglevel=info`: shows logs.

Note:

- Currently, we require 3 processes to run:

  - Django (main app)
  - Redis (message broker)
  - Celery (workers)

- Later, we'd be using Docker Compose to run everything with a single cmd:

  ```sh
  docker compose up
  ```

## Creating and Executing Tasks

1. **Create a Task**

   - In `playground/tasks.py`, define a dummy function `notify_customers(message)`.
   - Simulate long-running work with `time.sleep(10)`.
   - Print messages before/after to show progress.

2. **Make it a Celery Task**

   - Bad (tight coupling):

     ```py
     from storefront.celery import celery

     @celery.task
     def notify_customers(message)
     ```

     Problem: `playground` app now depends on `storefront`.

   - Better (decoupled):

     ```py
     from celery import shared_task

     @shared_task
     def notify_customers(message)
     ```

3. **Call the Task**

   - In `playground`, add a new view (and url):

     ```py
     from .tasks import notify_customers

     # Instead of `notify_customers("Hello")`, do:
     notify_customers.delay("Hello")
     ```

   - `.delay()` sends task to the broker instead of running it directly.

4. **Behavior**

   When you call the url:

   - Response returns **immediately** (non-blocking).
   - Task runs in background worker.

> While development, if we code a new task but the celery worker was already running, and we go to the url, then celery raises an error (`Received unregistered task`). So, we need to restart the celery worker after any new task addition.

6. **Offline Workers**

   - If Celery workers are offline:

     - No error in Django app.
     - And task message is still sent to broker (Redis).

   - When workers come back online:

     - Redis automatically delivers queued messages.
     - Workers execute the tasks.

   That's the benefit of using message broker, it ensures reliable delivery.

## Scheduling Periodic Tasks

Examples: periodic reports, sending emails, maintenance jobs.

- **Celery Beat**

  - Acts as a **scheduler/orchestrator**.
  - Kicks off tasks at defined times.
  - Actual task execution is done by **Celery workers**.

- **Configuration (in `settings.py`)**

  - Define `CELERY_BEAT_SCHEDULE` (dict of tasks).

  - Each task value (a dict itself) includes:

    - `task`: full dotted path to the task function.
    - `schedule`: how often to run.
    - (Optional) `args`: list for positional arguments.
    - (Optional) `kwargs`: dict for keyword arguments.

- **Schedule Options**

  - Simple interval: `5` → every 5 seconds; or `15*60` → every 15 minutes.

  - Advanced: use `crontab` (from `celery.schedules`).

    Example: `crontab(hour=7, minute=30, day_of_week=1)` → every Monday at 7:30 AM.

- **Running Celery Beat**

  - Make sure worker is running.

  - Start beat process separately (keep it running):

    ```sh
    celery --app storefront beat
    ```

  - This triggers tasks on schedule. (Use `schedule: 15` for testing.)

> Also, notice now we need to run 4 commands to completely start our project. (Docker Compose)

Docs: [Periodic Tasks](https://docs.celeryq.dev/en/stable/userguide/periodic-tasks.html), [Crontab schedules](https://docs.celeryq.dev/en/stable/userguide/periodic-tasks.html#crontab-schedules)

## Monitoring Celery Tasks

- **Purpose**: Monitor Celery tasks in real time.

- **Tool**: **`Flower`** (Pronunciation: `Flow-er`): Web-based Celery monitoring tool

  Celery Docs: [Flower: Real-time Celery web-monitor](https://docs.celeryq.dev/en/stable/userguide/monitoring.html#flower-real-time-celery-web-monitor)

- **Installation**:

  ```sh
  pipenv install flower
  ```

- **Start `Flower`**:

  ```sh
  celery --app storefront flower
  ```

  Access UI: [localhost:5555](http://localhost:5555)
